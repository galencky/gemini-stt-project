{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemini Speech-to-Text on Kaggle\n\nThis notebook transcribes audio files using Google's Gemini API with automatic chunking for long recordings. It processes medical audio with mixed Mandarin Chinese and English content.","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n!pip install --upgrade tqdm google-generativeai requests pydub","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport json\nimport shutil\nimport datetime\nimport time\nimport math\nfrom pathlib import Path\nfrom kaggle_secrets import UserSecretsClient\nfrom google.oauth2 import service_account\nfrom googleapiclient.discovery import build\nimport google.generativeai as genai\nfrom tqdm import tqdm\n\n# Get secrets\ns = UserSecretsClient()\n\n# Google Drive folder IDs\nto_be_transcribed = s.get_secret(\"TO_BE_TRANSCRIBED\")\ntranscribed = s.get_secret(\"TRANSCRIBED\")\nPROCESSED_ID = s.get_secret(\"PROCESSED\")\n\n# Gemini API key\nGEMINI_API_KEY = s.get_secret(\"GEMINI_API_KEY\")\nif not GEMINI_API_KEY:\n    raise ValueError(\"GEMINI_API_KEY not found in Kaggle secrets!\")\n\ngenai.configure(api_key=GEMINI_API_KEY)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up Google Drive authentication\nsa_json_str = s.get_secret(\"GDRIVE_SA_JSON\")\n\nsa_path = \"/kaggle/working/sa_key.json\"\nwith open(sa_path, \"w\") as f:\n    f.write(sa_json_str)\n\nSCOPES = [\"https://www.googleapis.com/auth/drive\"]\ncreds = service_account.Credentials.from_service_account_file(sa_path, scopes=SCOPES)\ndrive_service = build(\"drive\", \"v3\", credentials=creds)\n\n# Remove the local service account file for security\nif os.path.exists(sa_path):\n    try:\n        os.remove(sa_path)\n        print(f\"‚úÖ Removal successful: '{sa_path}' has been deleted.\")\n    except Exception as e:\n        print(f\"‚ùå Removal failed: {e}\")\n\n# Check for files in the to_be_transcribed folder\ndef list_files_in_folder(folder_id):\n    query = f\"'{folder_id}' in parents and trashed=false\"\n    files = []\n    page_token = None\n    while True:\n        resp = drive_service.files().list(\n            q=query,\n            spaces=\"drive\",\n            fields=\"nextPageToken, files(id, name)\",\n            pageToken=page_token\n        ).execute()\n        files.extend(resp.get(\"files\", []))\n        page_token = resp.get(\"nextPageToken\", None)\n        if not page_token:\n            break\n    return files\n\nfiles = list_files_in_folder(to_be_transcribed)\n\nif not files:\n    new_files = False\n    print(f\"‚ö†Ô∏è No files found in folder ID = {to_be_transcribed!r}. new_files = False.\")\nelse:\n    new_files = True\n    print(f\"‚úÖ Found {len(files)} file(s) in folder ID = {to_be_transcribed!r}. new_files = True.\")\n    for f in files:\n        print(f\" ‚Ä¢ {f['name']} (ID={f['id']})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download files from Google Drive\nimport io\nfrom googleapiclient.http import MediaIoBaseDownload\n\nTO_BE_TRANSCRIBED_ID = to_be_transcribed\n\nlocal_root = \"/kaggle/working/from_google_drive\"\nos.makedirs(local_root, exist_ok=True)\n\n# List all non-folder files\nquery_files = (\n    f\"'{TO_BE_TRANSCRIBED_ID}' in parents and \"\n    \"trashed = false and \"\n    \"mimeType != 'application/vnd.google-apps.folder'\"\n)\nresponse = drive_service.files().list(\n    q=query_files,\n    spaces=\"drive\",\n    fields=\"files(id, name)\"\n).execute()\nfiles = response.get(\"files\", [])\n\nif not files:\n    print(f\"‚ö†Ô∏è No files found in folder ID = {TO_BE_TRANSCRIBED_ID!r}, skip downloading.\")\nelse:\n    print(f\"üîé Found {len(files)} file(s) in folder ID = {TO_BE_TRANSCRIBED_ID!r}:\")\n    for f in files:\n        print(f\" ‚Ä¢ {f['name']}  (ID = {f['id']})\")\n\n# Download each file\nfor f in files:\n    file_id   = f[\"id\"]\n    file_name = f[\"name\"]\n    dest_path = os.path.join(local_root, file_name)\n\n    try:\n        request = drive_service.files().get_media(fileId=file_id)\n        fh = io.FileIO(dest_path, mode=\"wb\")\n        downloader = MediaIoBaseDownload(fh, request)\n        done = False\n        print(f\"‚¨áÔ∏è  Downloading {file_name!r} ‚Ä¶\")\n        while not done:\n            status, done = downloader.next_chunk()\n        fh.close()\n        print(f\"‚úÖ Saved to {dest_path}\")\n    except Exception as e:\n        print(f\"‚ùå Error downloading {file_name!r}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Audio processing utilities\nfrom pydub import AudioSegment\nimport subprocess\nimport tempfile\n\ndef get_audio_duration(audio_path):\n    \"\"\"Get audio duration in seconds using pydub.\"\"\"\n    try:\n        audio = AudioSegment.from_file(str(audio_path))\n        return len(audio) / 1000.0  # Convert milliseconds to seconds\n    except Exception as e:\n        print(f\"Error getting audio duration: {e}\")\n        return None\n\ndef split_audio_into_chunks(audio_path, chunk_duration_seconds=300):\n    \"\"\"Split audio file into chunks of specified duration.\"\"\"\n    chunks = []\n    try:\n        # Load audio\n        audio = AudioSegment.from_file(str(audio_path))\n        total_duration = len(audio) / 1000.0  # Convert to seconds\n        \n        print(f\"Total audio duration: {total_duration:.1f} seconds ({total_duration/60:.1f} minutes)\")\n        \n        # Calculate number of chunks\n        num_chunks = math.ceil(total_duration / chunk_duration_seconds)\n        \n        if num_chunks == 1:\n            print(\"Audio is shorter than chunk duration, processing as single file\")\n            return [audio_path]\n        \n        print(f\"Splitting audio into {num_chunks} chunks of {chunk_duration_seconds} seconds each...\")\n        \n        # Create temporary directory for chunks\n        temp_dir = Path(tempfile.mkdtemp())\n        \n        chunk_duration_ms = chunk_duration_seconds * 1000\n        \n        for i in range(num_chunks):\n            start_ms = i * chunk_duration_ms\n            end_ms = min((i + 1) * chunk_duration_ms, len(audio))\n            \n            # Extract chunk\n            chunk = audio[start_ms:end_ms]\n            \n            # Save chunk\n            chunk_path = temp_dir / f\"chunk_{i+1:03d}.wav\"\n            chunk.export(chunk_path, format=\"wav\")\n            \n            print(f\"Created chunk {i+1}/{num_chunks} (duration: {(end_ms-start_ms)/1000:.1f}s)\")\n            chunks.append(chunk_path)\n        \n        print(f\"Successfully created {len(chunks)} chunks\")\n        return chunks\n        \n    except Exception as e:\n        print(f\"Error splitting audio: {e}\")\n        return [audio_path]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gemini transcription functions\ndef transcribe_audio_chunk(chunk_path, chunk_number, total_chunks):\n    \"\"\"Transcribe a single audio chunk using Gemini API.\"\"\"\n    try:\n        print(f\"Transcribing chunk {chunk_number}/{total_chunks}...\")\n        \n        # Upload audio file to Gemini\n        audio_file = genai.upload_file(path=str(chunk_path), display_name=f\"chunk_{chunk_number}\")\n        \n        # Wait for file to be processed\n        while audio_file.state.name == \"PROCESSING\":\n            time.sleep(1)\n            audio_file = genai.get_file(audio_file.name)\n        \n        # Prepare the prompt\n        prompt = f\"\"\"You are transcribing chunk {chunk_number} of {total_chunks} from a medical audio recording that contains both Mandarin Chinese and Medical English terminology.\n\nCRITICAL INSTRUCTIONS:\n1. Transcribe the ENTIRE audio chunk from start to finish without stopping\n2. Include ALL content, even if there are pauses or quiet sections\n3. Do NOT summarize or skip any portions of the audio\n4. Continue transcribing until you reach the absolute end of this audio chunk\n5. This is chunk {chunk_number} of {total_chunks} - transcribe ONLY what you hear in this specific chunk\n\nLANGUAGE AND CONTENT:\n- The audio contains mixed Mandarin Chinese (Traditional Chinese zh-tw) and English medical terminology\n- Speakers may switch between languages mid-sentence\n- Medical terms may be spoken in English even within Chinese sentences\n- Include all medical terminology, abbreviations, and technical language exactly as spoken\n\nFORMAT:\n- Transcribe verbatim what is said\n- When speakers switch languages, transcribe in the language being spoken\n- Preserve medical terms in their original language (usually English)\n- Include speaker changes if distinguishable\n- Do NOT add any chunk markers or headers - just transcribe the content\n\nIMPORTANT: This is a complete transcription of chunk {chunk_number}. Transcribe every single word from the beginning to the end of this audio chunk.\"\"\"\n        \n        # Generate transcription\n        model = genai.GenerativeModel('gemini-2.5-pro')\n        response = model.generate_content([prompt, audio_file])\n        \n        # Delete uploaded file\n        genai.delete_file(audio_file.name)\n        \n        return response.text\n        \n    except Exception as e:\n        print(f\"Error transcribing chunk {chunk_number}: {e}\")\n        return None\n\ndef transcribe_audio_file(audio_path, chunk_duration_seconds=300):\n    \"\"\"Transcribe an audio file by splitting it into chunks.\"\"\"\n    try:\n        # Split audio into chunks\n        chunks = split_audio_into_chunks(audio_path, chunk_duration_seconds)\n        \n        if not chunks:\n            print(\"No chunks created, cannot proceed with transcription\")\n            return None\n        \n        # Transcribe each chunk\n        transcriptions = []\n        total_chunks = len(chunks)\n        \n        for i, chunk_path in enumerate(chunks, 1):\n            print(f\"\\n--- Processing chunk {i}/{total_chunks} ---\")\n            chunk_transcript = transcribe_audio_chunk(chunk_path, i, total_chunks)\n            \n            if chunk_transcript:\n                transcriptions.append(chunk_transcript)\n                print(f\"Successfully transcribed chunk {i}/{total_chunks}\")\n            else:\n                print(f\"Failed to transcribe chunk {i}/{total_chunks}\")\n        \n        # Clean up chunk files if they were created\n        if len(chunks) > 1:  # Only clean up if we created temporary chunks\n            temp_dir = chunks[0].parent\n            for chunk in chunks:\n                if chunk.exists():\n                    chunk.unlink()\n            if temp_dir.exists() and temp_dir != audio_path.parent:\n                temp_dir.rmdir()\n        \n        # Merge all transcriptions\n        if transcriptions:\n            print(f\"\\nMerging {len(transcriptions)} chunk transcriptions...\")\n            \n            # Join chunks with timestamps\n            merged_transcript = \"\"\n            for i, transcript in enumerate(transcriptions, 1):\n                timestamp = f\"[{(i-1)*chunk_duration_seconds//60:02d}:{(i-1)*chunk_duration_seconds%60:02d}:00.000]\"\n                merged_transcript += f\"{timestamp}\\n{transcript.strip()}\\n\\n\"\n            \n            return merged_transcript.strip()\n        else:\n            print(\"No chunks were successfully transcribed\")\n            return None\n            \n    except Exception as e:\n        print(f\"Error in chunked transcription: {e}\")\n        return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main transcription process\nAUDIO_EXT = {'.wav', '.mp3', '.m4a', '.flac', '.ogg', '.webm'}\nINBOX_DIR = Path(\"/kaggle/working/from_google_drive\")\nTRANSCRIPTS_DIR = Path(\"/kaggle/working/transcription\")\nCHUNK_DURATION_SECONDS = 300  # 5 minutes\n\n# Ensure directories exist\nTRANSCRIPTS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Gather audio files\naudio_files = [p for p in INBOX_DIR.rglob(\"*\") if p.suffix.lower() in AUDIO_EXT]\ntotal = len(audio_files)\n\nif total == 0:\n    print(f\"[{datetime.datetime.now():%Y-%m-%d %H:%M:%S}] üìÇ No audio files found under {INBOX_DIR}\")\n    print(\"Please place your audio files in /kaggle/working/from_google_drive and re-run.\")\n    new_files = False\nelse:\n    new_files = True\n\n# Transcription process\nif not new_files:\n    print(f\"[{datetime.datetime.now():%Y-%m-%d %H:%M:%S}] ‚ö†Ô∏è new_files == False ‚Üí Skipping transcription.\")\nelse:\n    print(f\"[{datetime.datetime.now():%Y-%m-%d %H:%M:%S}] üéß Found {total} audio file(s) under {INBOX_DIR}\")\n    \n    # Transcription loop with progress bar\n    with tqdm(audio_files, desc=\"Transcribing files\", unit=\"file\") as pbar:\n        for audio in pbar:\n            pbar.set_description(f\"Transcribing: {audio.name}\")\n            \n            # Transcribe using Gemini with chunking\n            transcript = transcribe_audio_file(audio, CHUNK_DURATION_SECONDS)\n            \n            if transcript:\n                # Save transcript\n                out_txt = TRANSCRIPTS_DIR / f\"{audio.stem}.txt\"\n                with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n                    f.write(transcript)\n                print(f\"\\n‚úÖ Saved transcript to: {out_txt}\")\n            else:\n                print(f\"\\n‚ùå Failed to transcribe {audio.name}\")\n    \n    print(f\"\\n[{datetime.datetime.now():%Y-%m-%d %H:%M:%S}] üéâ All transcription jobs finished! Transcripts are in {TRANSCRIPTS_DIR}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T07:01:06.509Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Parse transcripts into 5-minute blocks (similar to Whisper version)\nimport re\nfrom datetime import timedelta\n\n# Simple parser for Gemini output\ndef parse_transcript_simple(text: str) -> str:\n    \"\"\"Parse transcript into 5-minute blocks with preserved formatting.\"\"\"\n    lines = text.strip().split('\\n')\n    \n    result = []\n    current_block = []\n    \n    for line in lines:\n        line = line.strip()\n        if line.startswith('[') and line.endswith(']'):\n            # This is a timestamp line\n            if current_block:\n                result.append('\\n'.join(current_block))\n                result.append('')  # blank line\n                current_block = []\n            result.append(line)\n        elif line:\n            current_block.append(line)\n    \n    if current_block:\n        result.append('\\n'.join(current_block))\n    \n    return '\\n'.join(result)\n\n# Process transcripts\nPARSED_DIR = Path(\"/kaggle/working/parsed\")\nPARSED_DIR.mkdir(parents=True, exist_ok=True)\n\ntxt_files = list(TRANSCRIPTS_DIR.glob(\"*.txt\"))\nif not txt_files:\n    print(f\"[{datetime.datetime.now():%Y-%m-%d %H:%M:%S}] ‚ö†Ô∏è No .txt files found in {TRANSCRIPTS_DIR}. Skipping parsing.\")\nelse:\n    for txtfile in txt_files:\n        with txtfile.open(encoding=\"utf-8\") as f:\n            text = f.read()\n\n        processed = parse_transcript_simple(text)\n        out_path = PARSED_DIR / txtfile.name.replace(\".txt\", \"_parsed.txt\")\n\n        with out_path.open(\"w\", encoding=\"utf-8\") as f:\n            f.write(processed)\n        print(f\"üîß Processed {txtfile.name} ‚Üí {out_path.name}\")\n\n    print(\"\\n‚úÖ All transcripts parsed.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T07:01:06.510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get summary prompt from Google Doc (same as Whisper version)\nfrom googleapiclient.discovery import build\n\nDOC_ID = '1p44XUpBu7lPjyux4eANd_9FHT5F1UDbgUyx7q6Libvk'\n\ndef get_doc_text(doc_id: str, creds) -> str:\n    service = build('docs', 'v1', credentials=creds)\n    doc = service.documents().get(documentId=doc_id).execute()\n    text = []\n    for element in doc.get('body', {}).get('content', []):\n        if 'paragraph' in element:\n            for run in element['paragraph'].get('elements', []):\n                txt = run.get('textRun', {}).get('content')\n                if txt:\n                    text.append(txt)\n    return ''.join(text).strip()\n\n# Retrieve the system prompt from the Google Doc\nSYSTEM_PROMPT = get_doc_text(DOC_ID, creds)\nprint(\"[INFO] SYSTEM_PROMPT loaded from Google Doc. Preview:\\n\", SYSTEM_PROMPT[:200] + \"...\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T07:01:06.510Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate summaries using Gemini (same as Whisper version)\ndef generate_summary_with_gemini(speech_text: str, system_prompt: str) -> str:\n    model = genai.GenerativeModel(\"gemini-2.5-pro\")\n    full_prompt = system_prompt.strip() + \"\\n\\n\" + speech_text.strip()\n    try:\n        response = model.generate_content(\n            full_prompt,\n            generation_config=genai.types.GenerationConfig(temperature=0.5),\n            stream=False,\n        )\n        return response.text\n    except Exception as e:\n        print(f\"[ERROR] Gemini API error: {e}\")\n        return None\n\ndef process_all_txt_files(parsed_dir: Path, markdown_dir: Path, system_prompt: str):\n    parsed_dir = Path(parsed_dir)\n    markdown_dir = Path(markdown_dir)\n    markdown_dir.mkdir(parents=True, exist_ok=True)\n\n    txt_files = list(parsed_dir.glob(\"*.txt\"))\n    if not txt_files:\n        print(f\"[INFO] No parsed .txt files found in {parsed_dir}. Skipping summarization.\")\n        return\n\n    print(f\"[INFO] Found {len(txt_files)} .txt files in {parsed_dir}\")\n\n    for txt_path in txt_files:\n        print(f\"\\n[INFO] Processing: {txt_path.name}\")\n        try:\n            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n                speech_text = f.read().strip()\n        except Exception as e:\n            print(f\"[ERROR] Could not read {txt_path}: {e}\")\n            continue\n\n        if not speech_text:\n            print(f\"[WARNING] {txt_path.name} is empty, skipping.\")\n            continue\n\n        summary_md = generate_summary_with_gemini(speech_text, system_prompt)\n        if summary_md is None:\n            print(f\"[ERROR] Gemini API failed for {txt_path.name}, skipping.\")\n            continue\n\n        md_path = markdown_dir / (txt_path.stem.replace(\"_parsed\", \"\") + \".md\")\n\n        try:\n            with open(md_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(summary_md)\n            print(f\"[INFO] Saved summary ‚Üí {md_path.name}\")\n        except Exception as e:\n            print(f\"[ERROR] Could not save {md_path}: {e}\")\n\n# Set directories\nPARSED_DIR = Path(\"/kaggle/working/parsed\")\nMARKDOWN_DIR = Path(\"/kaggle/working/markdown\")\n\n# Only run if there are files to process\nif list(PARSED_DIR.glob(\"*.txt\")):\n    process_all_txt_files(PARSED_DIR, MARKDOWN_DIR, SYSTEM_PROMPT)\n    print(\"\\n‚úÖ All summaries generated.\")\nelse:\n    print(f\"[INFO] No .txt files found in {PARSED_DIR}, nothing to summarize.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T07:01:06.510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Upload to HackMD (same as Whisper version)\nimport requests\n\nhackmd_token = s.get_secret(\"HACKMD_TOKEN\")\nif hackmd_token is None:\n    raise ValueError(\"HACKMD_TOKEN not found in Kaggle secrets!\")\n\ndef upload_to_hackmd(md_content: str, filename: str, api_token: str) -> dict:\n    \"\"\"Uploads a single markdown string to HackMD.\"\"\"\n    # Derive a clean title from the filename\n    if filename.endswith('.md'):\n        filename = filename[:-3]\n    raw_title = filename.replace('_parsed', '').strip()\n    title = raw_title.replace('_', ' ').strip()\n\n    # Ensure there's a top-level heading\n    md_lines = md_content.lstrip().splitlines()\n    if not md_lines or not md_lines[0].strip().startswith(\"# \"):\n        md_content = f\"# {title}\\n\\n\" + md_content.lstrip()\n    else:\n        md_lines[0] = f\"# {title}\"\n        md_content = \"\\n\".join(md_lines)\n\n    # Append hashtag\n    hashtag = \"#gemini-stt-project\"\n    content_lines = md_content.rstrip().splitlines()\n    if not any(line.strip() == hashtag for line in content_lines[-3:]):\n        md_content = md_content.rstrip() + \"\\n\\n\" + hashtag + \"\\n\"\n\n    url = \"https://api.hackmd.io/v1/notes\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_token}\",\n        \"Content-Type\": \"application/json\"\n    }\n    data = {\n        \"title\": title,\n        \"content\": md_content,\n        \"readPermission\": \"guest\",\n        \"writePermission\": \"signed_in\"\n    }\n    response = requests.post(url, headers=headers, json=data)\n    if response.ok:\n        note_id = response.json().get(\"id\")\n        shared_url = f\"https://hackmd.io/{note_id}\"\n        print(f\"[INFO] Uploaded to HackMD: {shared_url}\")\n        return {\"title\": title, \"url\": shared_url}\n    else:\n        print(f\"[ERROR] HackMD upload failed for {filename}: {response.status_code} {response.text}\")\n        return None\n\ndef batch_upload_markdown_and_move(markdown_dir: Path, uploaded_dir: Path, hackmd_token: str) -> list:\n    markdown_dir = Path(markdown_dir)\n    uploaded_dir = Path(uploaded_dir)\n    uploaded_dir.mkdir(parents=True, exist_ok=True)\n\n    md_files = list(markdown_dir.glob(\"*.md\"))\n    if not md_files:\n        print(f\"[INFO] No markdown files found in {markdown_dir}, skipping upload.\")\n        return []\n\n    print(f\"[INFO] Found {len(md_files)} markdown files to upload.\")\n\n    shared_links = []\n    for md_file in md_files:\n        print(f\"[INFO] Processing: {md_file.name}\")\n        try:\n            with open(md_file, \"r\", encoding=\"utf-8\") as f:\n                md_content = f.read()\n        except Exception as e:\n            print(f\"[ERROR] Could not read {md_file.name}: {e}\")\n            continue\n\n        result = upload_to_hackmd(md_content, md_file.name, hackmd_token)\n        if result:\n            shared_links.append(result)\n            dest_file = uploaded_dir / md_file.name\n            try:\n                shutil.move(str(md_file), dest_file)\n                print(f\"[INFO] Moved {md_file.name} ‚Üí {dest_file}\")\n            except Exception as e:\n                print(f\"[ERROR] Failed to move {md_file.name}: {e}\")\n    return shared_links\n\n# Upload to HackMD\nMARKDOWN_DIR = Path(\"/kaggle/working/markdown\")\nUPLOADED_DIR = Path(\"/kaggle/working/uploaded\")\n\nif list(MARKDOWN_DIR.glob(\"*.md\")):\n    shared_links = batch_upload_markdown_and_move(MARKDOWN_DIR, UPLOADED_DIR, hackmd_token)\n    print(\"\\n‚úÖ All markdown files uploaded to HackMD.\")\nelse:\n    print(f\"[INFO] No .md files found in {MARKDOWN_DIR}, nothing to upload.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T07:01:06.510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\nfrom pathlib import Path\nimport datetime\n\n# Replace with your secret retrieval method\nINBOX_ID = s.get_secret(\"TO_BE_TRANSCRIBED\")\nARCHIVE_ID = s.get_secret(\"TRANSCRIBED\")\nPROCESSED_ID = s.get_secret(\"PROCESSED\")\n\n# Local paths\nWORKING = Path(\"/kaggle/working\")\nTRANS_DIR = WORKING / \"transcription\"\nPARSED_DIR = WORKING / \"parsed\"\nUPLOADED_MD = WORKING / \"uploaded\"\nAUDIO_LOCAL = WORKING / \"from_google_drive\"\n\n# Build drive service\ndrive = build(\"drive\", \"v3\", credentials=creds)\n\ndef log(msg): \n    print(f\"[{datetime.datetime.now():%H:%M:%S}] {msg}\")\n\ndef ensure_subfolder(parent_id: str, name: str) -> str:\n    \"\"\"Return id of subfolder 'name' under parent, creating if absent.\"\"\"\n    q = (f\"'{parent_id}' in parents and mimeType='application/vnd.google-apps.folder' \"\n         f\"and name='{name}' and trashed=false\")\n    res = drive.files().list(\n        q=q,\n        spaces=\"drive\",\n        fields=\"files(id)\",\n        supportsAllDrives=True\n    ).execute()\n    if res[\"files\"]:\n        return res[\"files\"][0][\"id\"]\n    meta = {\n        \"name\": name,\n        \"mimeType\": \"application/vnd.google-apps.folder\",\n        \"parents\": [parent_id]\n    }\n    return drive.files().create(\n        body=meta,\n        fields=\"id\",\n        supportsAllDrives=True\n    ).execute()[\"id\"]\n\ndef upload_file(local: Path, parent_id: str):\n    media = MediaFileUpload(local, resumable=False)\n    meta  = {\"name\": local.name, \"parents\": [parent_id]}\n    drive.files().create(\n        body=meta,\n        media_body=media,\n        fields=\"id\",\n        supportsAllDrives=True\n    ).execute()\n    log(f\"  ‚Ü≥ uploaded {local.name}\")\n\ndef move_audio(audio_name: str):\n    \"\"\"Move audio file from inbox to archive.\"\"\"\n    q = f\"'{INBOX_ID}' in parents and name='{audio_name}' and trashed=false\"\n    res = drive.files().list(\n        q=q,\n        spaces=\"drive\",\n        fields=\"files(id)\",\n        supportsAllDrives=True\n    ).execute().get(\"files\", [])\n    \n    if not res:\n        return\n    fid = res[0][\"id\"]\n    drive.files().update(\n        fileId=fid,\n        addParents=ARCHIVE_ID,\n        removeParents=INBOX_ID,\n        fields=\"id\",\n        supportsAllDrives=True\n    ).execute()\n    log(f\"  ‚Ü≥ moved {audio_name} ‚Üí transcribed\")\n\n# Process markdown files\nmd_files = list(UPLOADED_MD.glob(\"*.md\"))\nif not md_files:\n    log(\"‚ÑπÔ∏è  No markdown files in /uploaded ‚Äì nothing to sync.\")\nelse:\n    for md in md_files:\n        stem = md.stem\n        folder_id = ensure_subfolder(PROCESSED_ID, stem)\n        log(f\"üìÇ Drive subfolder '{stem}' (id {folder_id})\")\n\n        txt_path    = TRANS_DIR  / f\"{stem}.txt\"\n        parsed_path = PARSED_DIR / f\"{stem}_parsed.txt\"\n\n        for p in (txt_path, parsed_path, md):\n            if p.exists():\n                upload_file(p, folder_id)\n\n        # Move corresponding audio\n        for audio_local in AUDIO_LOCAL.glob(f\"{stem}.*\"):\n            if audio_local.is_file():\n                move_audio(audio_local.name)\n                break\n\n        # Verify contents\n        present = {f[\"name\"] for f in drive.files().list(\n            q=f\"'{folder_id}' in parents and trashed=false\",\n            spaces=\"drive\",\n            fields=\"files(name)\",\n            supportsAllDrives=True\n        ).execute()[\"files\"]}\n        expected = {txt_path.name, parsed_path.name, md.name}\n        if expected - present:\n            log(f\"  ‚úñ missing {expected - present}\")\n        else:\n            log(\"  ‚úÖ files verified\")\n\n        md.unlink(missing_ok=True)\n\n# Move any remaining audio files\nfor audio_local in AUDIO_LOCAL.glob(\"*\"):\n    if audio_local.is_file():\n        move_audio(audio_local.name)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T07:01:06.510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Email HackMD links (same as Whisper version)\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom email.header import Header\n\n# Skip if no links were produced\nif not (globals().get(\"shared_links\") and shared_links):\n    print(\"[INFO] No uploaded Markdown links ‚Äì skipping email step.\")\nelse:\n    # Retrieve email secrets\n    email_user = s.get_secret(\"EMAIL_USER\")\n    email_pass = s.get_secret(\"EMAIL_PASS\")\n    email_to   = s.get_secret(\"EMAIL_TO\")\n\n    if not all([email_user, email_pass, email_to]):\n        print(\"[WARN] Email secrets missing ‚Äì email not sent.\")\n    else:\n        # Build email body\n        subject = \"üìù Your Uploaded HackMD Speech Summaries (Gemini STT)\"\n        body_lines = [\n            \"Hello,\",\n            \"\",\n            \"Your audio files were transcribed using Gemini STT with chunking\",\n            \"and summarized using Gemini 2.5 Pro. The summaries are now\",\n            \"available on HackMD:\",\n            \"\"\n        ] + [f\"- {link['title']}: {link['url']}\" for link in shared_links] + [\n            \"\",\n            \"If you have questions just reply to this email.\",\n            \"\",\n            \"Best regards,\",\n            \"Gemini-STT Bot\"\n        ]\n        body = \"\\n\".join(body_lines)\n\n        # Compose & send\n        msg             = MIMEMultipart()\n        msg[\"From\"]     = email_user\n        msg[\"To\"]       = email_to\n        msg[\"Subject\"]  = Header(subject, \"utf-8\")\n        msg.attach(MIMEText(body, \"plain\", \"utf-8\"))\n\n        try:\n            with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as server:\n                server.login(email_user, email_pass)\n                server.send_message(msg)\n            print(\"[INFO] Email sent successfully.\")\n        except Exception as e:\n            print(f\"[ERROR] Email send failed: {e}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T07:01:06.510Z"}},"outputs":[],"execution_count":null}]}